{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_interests = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 4),\n",
       " ('R', 4),\n",
       " ('Big Data', 3),\n",
       " ('HBase', 3),\n",
       " ('Java', 3),\n",
       " ('statistics', 3),\n",
       " ('regression', 3),\n",
       " ('probability', 3),\n",
       " ('Hadoop', 2),\n",
       " ('Cassandra', 2),\n",
       " ('MongoDB', 2),\n",
       " ('Postgres', 2),\n",
       " ('scikit-learn', 2),\n",
       " ('statsmodels', 2),\n",
       " ('pandas', 2),\n",
       " ('machine learning', 2),\n",
       " ('libsvm', 2),\n",
       " ('C++', 2),\n",
       " ('neural networks', 2),\n",
       " ('deep learning', 2),\n",
       " ('artificial intelligence', 2),\n",
       " ('Spark', 1),\n",
       " ('Storm', 1),\n",
       " ('NoSQL', 1),\n",
       " ('scipy', 1),\n",
       " ('numpy', 1),\n",
       " ('decision trees', 1),\n",
       " ('Haskell', 1),\n",
       " ('programming languages', 1),\n",
       " ('mathematics', 1),\n",
       " ('theory', 1),\n",
       " ('Mahout', 1),\n",
       " ('MapReduce', 1),\n",
       " ('databases', 1),\n",
       " ('MySQL', 1),\n",
       " ('support vector machines', 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recommending the most popular\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "popular_interests = Counter(interest\n",
    "                            for user_interests in users_interests\n",
    "                            for interest in user_interests)\n",
    "\n",
    "popular_interests.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def most_popular_new_interests(\n",
    "    user_interests: List[str], \n",
    "    max_results: int = 5) -> List[Tuple[str, int]]:\n",
    "    \n",
    "    suggestions = [(interest, frequency) \n",
    "                   for interest, frequency in popular_interests.most_common() \n",
    "                   if interest not in user_interests]\n",
    "    \n",
    "    return suggestions[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 4), ('R', 4), ('Big Data', 3), ('Java', 3), ('statistics', 3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_popular_new_interests([\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Big Data', 3), ('HBase', 3), ('Java', 3), ('Hadoop', 2), ('Cassandra', 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_popular_new_interests([\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# User-based Collaborative Filtering\n",
    "\n",
    "unique_interests = sorted({interest\n",
    "                           for user_interests in users_interests \n",
    "                           for interest in user_interests})\n",
    "\n",
    "assert unique_interests[:6] == [\n",
    "    'Big Data',\n",
    "    'C++',\n",
    "    'Cassandra',\n",
    "    'HBase',\n",
    "    'Hadoop',\n",
    "    'Haskell'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User feature\n",
    "\n",
    "def make_user_interest_vector(user_interests: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Given a list of interests, produce a vector whose ith element is 1\n",
    "    if unique_interests[i] is in the list, 0 otherwise\n",
    "    \"\"\"\n",
    "    return [1 if interest in user_interests else 0\n",
    "           for interest in unique_interests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interest_vectors = [make_user_interest_vector(user_interests)\n",
    "                         for user_interests in users_interests] # user-item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1725.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Java 3\n",
      "0 Big Data 3\n",
      "0 Hadoop 2\n",
      "0 HBase 1\n",
      "0 C++ 1\n",
      "0 Spark 1\n",
      "0 Storm 1\n",
      "0 programming languages 1\n",
      "0 MapReduce 1\n",
      "0 Cassandra 1\n",
      "0 deep learning 1\n",
      "1 HBase 2\n",
      "1 neural networks 2\n",
      "1 Postgres 2\n",
      "1 MongoDB 2\n",
      "1 machine learning 2\n",
      "1 Cassandra 1\n",
      "1 numpy 1\n",
      "1 decision trees 1\n",
      "1 deep learning 1\n",
      "1 databases 1\n",
      "1 MySQL 1\n",
      "1 NoSQL 1\n",
      "1 artificial intelligence 1\n",
      "1 scipy 1\n",
      "2 regression 3\n",
      "2 Python 2\n",
      "2 R 2\n",
      "2 libsvm 2\n",
      "2 scikit-learn 2\n",
      "2 mathematics 1\n",
      "2 support vector machines 1\n",
      "2 Haskell 1\n",
      "2 Mahout 1\n",
      "3 statistics 3\n",
      "3 probability 3\n",
      "3 Python 2\n",
      "3 R 2\n",
      "3 pandas 2\n",
      "3 statsmodels 2\n",
      "3 C++ 1\n",
      "3 artificial intelligence 1\n",
      "3 theory 1\n",
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "Big Data and programming languages 7\n",
      "\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "Python and statistics 5\n",
      "\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "Python and statistics 2\n",
      "databases 2\n",
      "machine learning 2\n",
      "\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "machine learning 3\n",
      "databases 2\n",
      "\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "databases 2\n",
      "Python and statistics 2\n",
      "\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "databases 3\n",
      "Big Data and programming languages 3\n",
      "\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "machine learning 3\n",
      "databases 1\n",
      "\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "databases 2\n",
      "Python and statistics 2\n",
      "\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "Python and statistics 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Big Data and programming languages 4\n",
      "\n",
      "['statistics', 'R', 'statsmodels']\n",
      "machine learning 3\n",
      "\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "machine learning 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['pandas', 'R', 'Python']\n",
      "machine learning 3\n",
      "\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "Python and statistics 5\n",
      "\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "databases 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pair-wise user similarities\n",
    "\n",
    "from scratch.nlp import cosine_similarity\n",
    "\n",
    "user_similarities = [[cosine_similarity(interest_vector_i, interest_vector_j) \n",
    "                      for interest_vector_j in user_interest_vectors] \n",
    "                     for interest_vector_i in user_interest_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.3380617018914066,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1543033499620919,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1889822365046136,\n",
       " 0.5669467095138409,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1690308509457033,\n",
       " 0.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users 0 and 9 share interests in Hadoop, Java, and Big Data\n",
    "assert 0.56 < user_similarities[0][9] < 0.58, \"several shared interests\"\n",
    "\n",
    "# Users 0 and 8 share only one interest: Big Data\n",
    "assert 0.18 < user_similarities[0][8] < 0.20, \"only one shared interest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_users_to(user_id: int) -> List[Tuple[int, float]]:\n",
    "    pairs = [(other_user_id, similarity) \n",
    "             for other_user_id, similarity in enumerate(user_similarities[user_id]) \n",
    "             if user_id != other_user_id and similarity > 0]\n",
    "    \n",
    "    return sorted(pairs, \n",
    "                  key=lambda pair: pair[-1], \n",
    "                  reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 0.5669467095138409),\n",
       " (1, 0.3380617018914066),\n",
       " (8, 0.1889822365046136),\n",
       " (13, 0.1690308509457033),\n",
       " (5, 0.1543033499620919)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_users_to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def user_based_suggestions(user_id: int, \n",
    "                           include_current_interests: bool = False):\n",
    "    \n",
    "    # Sum up the user similarities for each interest\n",
    "    suggestions: Dict[str, float] = defaultdict(float)\n",
    "        \n",
    "    for other_user_id, similarity in most_similar_users_to(user_id):\n",
    "        for interest in users_interests[other_user_id]:\n",
    "            suggestions[interest] += similarity # more weight for a specific topic interested by users who similar to the current user\n",
    "            \n",
    "    # Convert them to a sorted list\n",
    "    suggestions = sorted(suggestions.items(), \n",
    "                         key=lambda pair: pair[-1], \n",
    "                         reverse=True)\n",
    "    \n",
    "    # And exclude already interests\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight) \n",
    "                for suggestion, weight in suggestions \n",
    "                if suggestion not in users_interests[user_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MapReduce', 0.5669467095138409),\n",
       " ('MongoDB', 0.50709255283711),\n",
       " ('Postgres', 0.50709255283711),\n",
       " ('NoSQL', 0.3380617018914066),\n",
       " ('neural networks', 0.1889822365046136),\n",
       " ('deep learning', 0.1889822365046136),\n",
       " ('artificial intelligence', 0.1889822365046136),\n",
       " ('databases', 0.1690308509457033),\n",
       " ('MySQL', 0.1690308509457033),\n",
       " ('Python', 0.1543033499620919),\n",
       " ('R', 0.1543033499620919),\n",
       " ('C++', 0.1543033499620919),\n",
       " ('Haskell', 0.1543033499620919),\n",
       " ('programming languages', 0.1543033499620919)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_based_suggestions(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-based Collaborative Filtering\n",
    "\n",
    "interest_user_matrix = [[user_interest_vector[j] \n",
    "                         for user_interest_vector in user_interest_vectors] \n",
    "                        for j, _ in enumerate(unique_interests)] # item-user matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interest_user_matrix # row: interests, column: users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair-wise item similarities\n",
    "\n",
    "interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j) \n",
    "                          for user_vector_j in interest_user_matrix] \n",
    "                         for user_vector_i in interest_user_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.0,\n",
       " 0.4082482904638631,\n",
       " 0.3333333333333333,\n",
       " 0.8164965809277261,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.5773502691896258,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5773502691896258,\n",
       " 0.5773502691896258,\n",
       " 0.4082482904638631,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4082482904638631,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4082482904638631,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interest_similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_interests_to(interest_id: int):\n",
    "    similarities = interest_similarities[interest_id]\n",
    "    \n",
    "    pairs = [(unique_interests[other_interest_id], similarity) \n",
    "             for other_interest_id, similarity in enumerate(similarities) \n",
    "             if interest_id != other_interest_id and similarity > 0]\n",
    "    \n",
    "    return sorted(pairs, \n",
    "                  key=lambda pair: pair[-1], \n",
    "                  reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hadoop', 0.8164965809277261),\n",
       " ('Java', 0.6666666666666666),\n",
       " ('MapReduce', 0.5773502691896258),\n",
       " ('Spark', 0.5773502691896258),\n",
       " ('Storm', 0.5773502691896258),\n",
       " ('Cassandra', 0.4082482904638631),\n",
       " ('artificial intelligence', 0.4082482904638631),\n",
       " ('deep learning', 0.4082482904638631),\n",
       " ('neural networks', 0.4082482904638631),\n",
       " ('HBase', 0.3333333333333333)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_interests_to(0) # Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_suggestions(user_id: int, \n",
    "                           include_current_interests: bool = False):\n",
    "    \n",
    "    # Add up the similar interests\n",
    "    suggestions = defaultdict(float)\n",
    "    user_interest_vector = user_interest_vectors[user_id]\n",
    "    \n",
    "    for interest_id, is_interested in enumerate(user_interest_vector):\n",
    "        if is_interested == 1: # a specific topic interested by this user\n",
    "            similar_interests = most_similar_interests_to(interest_id)\n",
    "            \n",
    "            for interest, similarity in similar_interests:\n",
    "                suggestions[interest] += similarity\n",
    "                \n",
    "    # Sort them by weight\n",
    "    suggestions = sorted(suggestions.items(), \n",
    "                         key=lambda pair: pair[-1], \n",
    "                         reverse=True)\n",
    "    \n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion, weight) \n",
    "                for suggestion, weight in suggestions \n",
    "                if suggestion not in users_interests[user_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MapReduce', 1.861807319565799),\n",
       " ('MongoDB', 1.3164965809277263),\n",
       " ('Postgres', 1.3164965809277263),\n",
       " ('NoSQL', 1.2844570503761732),\n",
       " ('MySQL', 0.5773502691896258),\n",
       " ('databases', 0.5773502691896258),\n",
       " ('Haskell', 0.5773502691896258),\n",
       " ('programming languages', 0.5773502691896258),\n",
       " ('artificial intelligence', 0.4082482904638631),\n",
       " ('deep learning', 0.4082482904638631),\n",
       " ('neural networks', 0.4082482904638631),\n",
       " ('C++', 0.4082482904638631),\n",
       " ('Python', 0.2886751345948129),\n",
       " ('R', 0.2886751345948129)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_based_suggestions(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "MOVIES = \"data\\\\ml-100k\\\\u.item\"  # pipe-delimited: movie_id|title|...\n",
    "RATINGS = \"data\\\\ml-100k\\\\u.data\" # tab-delimited: user_id, movie_id, rating, timestamp\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Rating(NamedTuple):\n",
    "    user_id: str\n",
    "    movie_id: str\n",
    "    rating: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(MOVIES, encoding=\"iso-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"|\")\n",
    "    movies = {movie_id: title for movie_id, title, *_ in reader}\n",
    "    \n",
    "with open(RATINGS, encoding=\"iso-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    ratings = [Rating(user_id, movie_id, float(rating)) \n",
    "               for user_id, movie_id, rating, _ in reader]\n",
    "    \n",
    "assert len(movies) == 1682\n",
    "assert len(list({rating.user_id for rating in ratings})) == 943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36 Star Wars (1977)\n",
      "4.20 Empire Strikes Back, The (1980)\n",
      "4.01 Return of the Jedi (1983)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Data structure for accumulating ratings by movie_id\n",
    "star_wars_ratings = {movie_id: [] \n",
    "                     for movie_id, title in movies.items() \n",
    "                     if re.search(\"Star Wars|Empire Strikes|Jedi\", title)}\n",
    "\n",
    "# Iterate over ratings, accumulating the Star Wars ones\n",
    "for rating in ratings:\n",
    "    if rating.movie_id in star_wars_ratings:\n",
    "        star_wars_ratings[rating.movie_id].append(rating.rating)\n",
    "        \n",
    "# Compute the average rating for each movie\n",
    "avg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id) \n",
    "               for movie_id, title_ratings \n",
    "               in star_wars_ratings.items()]\n",
    "\n",
    "# And then print them in order\n",
    "for avg_rating, movie_id in sorted(avg_ratings, reverse=True):\n",
    "    print(f\"{avg_rating:.2f} {movies[movie_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "random.shuffle(ratings)\n",
    "\n",
    "split1 = int(len(ratings) * 0.7)\n",
    "split2 = int(len(ratings) * 0.85)\n",
    "\n",
    "train = ratings[:split1]            # 70% train\n",
    "validation = ratings[split1:split2] # 15% validation\n",
    "test = ratings[split2:]             # 15% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "\n",
    "avg_rating = sum(rating.rating for rating in train) / len(train)\n",
    "baseline_error = sum((rating.rating - avg_rating) ** 2 \n",
    "                     for rating in test) / len(test)\n",
    "\n",
    "assert 1.26 < baseline_error < 1.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User and item embeddings\n",
    "\n",
    "from scratch.deep_learning import random_tensor\n",
    "\n",
    "EMBEDDING_DIM = 2\n",
    "\n",
    "# Find unique ids\n",
    "user_ids = {rating.user_id for rating in ratings}\n",
    "movie_ids = {rating.movie_id for rating in ratings}\n",
    "\n",
    "# Then create a random vector per id\n",
    "user_vectors = {user_id: random_tensor(EMBEDDING_DIM) \n",
    "                for user_id in user_ids}\n",
    "movie_vectors = {movie_id: random_tensor(EMBEDDING_DIM) \n",
    "                 for movie_id in movie_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import tqdm\n",
    "from scratch.linear_algebra import dot\n",
    "\n",
    "def loop(dataset: List[Rating],\n",
    "         learning_rate: float = None) -> None:\n",
    "    \n",
    "    with tqdm.tqdm(dataset) as t:\n",
    "        loss = 0.0\n",
    "\n",
    "        for i, rating in enumerate(t):\n",
    "            movie_vector = movie_vectors[rating.movie_id]\n",
    "            user_vector = user_vectors[rating.user_id]\n",
    "            predicted = dot(user_vector, movie_vector)\n",
    "            error = predicted - rating.rating\n",
    "            loss += error ** 2\n",
    "\n",
    "            if learning_rate is not None:\n",
    "                #     predicted = m_0 * u_0 + ... + m_k * u_k\n",
    "                # So each u_j enters output with coefficent m_j\n",
    "                # and each m_j enters output with coefficient u_j\n",
    "                user_gradient = [error * m_j for m_j in movie_vector]\n",
    "                movie_gradient = [error * u_j for u_j in user_vector]\n",
    "\n",
    "                # Take gradient steps\n",
    "                for j in range(EMBEDDING_DIM):\n",
    "                    user_vector[j] -= learning_rate * user_gradient[j]\n",
    "                    movie_vector[j] -= learning_rate * movie_gradient[j]\n",
    "\n",
    "            t.set_description(f\"avg loss: {loss / (i + 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 15.107575888126913:   0%|                                           | 144/70000 [00:00<00:48, 1429.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.045000000000000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 5.537957207704006: 100%|██████████████████████████████████████████| 70000/70000 [01:08<00:00, 1028.66it/s]\n",
      "avg loss: 1.1123773554311467: 100%|█████████████████████████████████████████| 70000/70000 [01:08<00:00, 1023.20it/s]\n",
      "avg loss: 1.039478084619848:   0%|                                            | 101/70000 [00:00<01:09, 1002.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.03645000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 1.0604769288364837: 100%|█████████████████████████████████████████| 15000/15000 [00:14<00:00, 1023.36it/s]\n",
      "avg loss: 0.9375644393738976:   0%|                                           | 120/70000 [00:00<00:58, 1197.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.03280500000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9463531368830601: 100%|█████████████████████████████████████████| 70000/70000 [01:06<00:00, 1046.07it/s]\n",
      "avg loss: 1.014313485601622: 100%|██████████████████████████████████████████| 15000/15000 [00:14<00:00, 1018.86it/s]\n",
      "avg loss: 0.9172486666146775:   0%|                                           | 118/70000 [00:00<00:59, 1171.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.02657205000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9987568789969326: 100%|██████████████████████████████████████████| 15000/15000 [00:17<00:00, 848.98it/s]\n",
      "avg loss: 0.9744006166172386:   0%|                                             | 70/70000 [00:00<01:40, 694.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.02391484500000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.904328601136122: 100%|███████████████████████████████████████████| 70000/70000 [01:40<00:00, 697.95it/s]\n",
      "avg loss: 0.9858609979879283: 100%|██████████████████████████████████████████| 15000/15000 [00:17<00:00, 875.84it/s]\n",
      "avg loss: 0.916436834213724:   0%|                                              | 71/70000 [00:00<01:40, 697.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.021523360500000012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9750411764474866: 100%|██████████████████████████████████████████| 15000/15000 [00:16<00:00, 902.44it/s]\n",
      "avg loss: 0.9088727629190797:   0%|                                             | 87/70000 [00:00<01:20, 863.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.01937102445000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8735983701733205: 100%|██████████████████████████████████████████| 70000/70000 [01:10<00:00, 986.60it/s]\n",
      "avg loss: 0.9659153713042742: 100%|█████████████████████████████████████████| 15000/15000 [00:14<00:00, 1008.59it/s]\n",
      "avg loss: 0.8931339527241323:   0%|                                            | 100/70000 [00:00<01:10, 992.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.01743392200500001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8612490183996377: 100%|██████████████████████████████████████████| 70000/70000 [01:11<00:00, 978.68it/s]\n",
      "avg loss: 0.9581907321267783: 100%|█████████████████████████████████████████| 15000/15000 [00:14<00:00, 1010.83it/s]\n",
      "avg loss: 0.8480710808188429:   0%|                                           | 104/70000 [00:00<01:07, 1032.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.015690529804500006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8504941221161235: 100%|██████████████████████████████████████████| 70000/70000 [01:11<00:00, 978.54it/s]\n",
      "avg loss: 0.9516311017689516: 100%|██████████████████████████████████████████| 15000/15000 [00:15<00:00, 989.29it/s]\n",
      "avg loss: 0.8180949944258382:   0%|                                           | 118/70000 [00:00<00:59, 1171.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0.014121476824050006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8410854443535845: 100%|██████████████████████████████████████████| 70000/70000 [01:10<00:00, 995.20it/s]\n",
      "avg loss: 0.9460429187170514: 100%|██████████████████████████████████████████| 15000/15000 [00:15<00:00, 973.42it/s]\n",
      "avg loss: 0.83420923256124:   0%|                                             | 108/70000 [00:00<01:05, 1072.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.012709329141645007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8328156276873361: 100%|██████████████████████████████████████████| 70000/70000 [01:10<00:00, 993.26it/s]\n",
      "avg loss: 0.9412662597999573: 100%|██████████████████████████████████████████| 15000/15000 [00:15<00:00, 997.54it/s]\n",
      "avg loss: 0.8273489010291141:   0%|                                             | 98/70000 [00:00<01:11, 972.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.011438396227480507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8255126209530377: 100%|██████████████████████████████████████████| 70000/70000 [01:11<00:00, 982.69it/s]\n",
      "avg loss: 0.9371684482957946: 100%|█████████████████████████████████████████| 15000/15000 [00:14<00:00, 1002.38it/s]\n",
      "avg loss: 0.8073368872299327:   0%|                                           | 107/70000 [00:00<01:05, 1062.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.010294556604732457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8190342347779098: 100%|██████████████████████████████████████████| 70000/70000 [01:13<00:00, 956.64it/s]\n",
      "avg loss: 0.9336393647725846: 100%|██████████████████████████████████████████| 15000/15000 [00:16<00:00, 920.11it/s]\n",
      "avg loss: 0.8138662819120622:   0%|                                           | 114/70000 [00:00<01:01, 1131.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0.00926510094425921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8132630397977435: 100%|██████████████████████████████████████████| 70000/70000 [01:12<00:00, 962.51it/s]\n",
      "avg loss: 0.9305878797342692: 100%|██████████████████████████████████████████| 15000/15000 [00:15<00:00, 950.11it/s]\n",
      "avg loss: 0.8367521754692008:   0%|                                           | 101/70000 [00:00<01:09, 1002.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 0.00833859084983329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.8081018814249854: 100%|██████████████████████████████████████████| 70000/70000 [01:13<00:00, 958.34it/s]\n",
      "avg loss: 0.9256312699003789: 100%|██████████████████████████████████████████| 15000/15000 [00:15<00:00, 939.83it/s]\n",
      "avg loss: 0.8308758294856923:   0%|                                             | 93/70000 [00:00<01:16, 914.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 0.006754258588364966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.9236147530743195: 100%|██████████████████████████████████████████| 15000/15000 [00:16<00:00, 920.11it/s]\n",
      "avg loss: 0.7807713825637939:   0%|                                           | 126/70000 [00:00<00:55, 1250.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0.00607883272952847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: 0.7955369240876724: 100%|██████████████████████████████████████████| 70000/70000 [01:12<00:00, 966.95it/s]\n",
      "avg loss: 0.9218487100096145: 100%|██████████████████████████████████████████| 15000/15000 [00:16<00:00, 934.26it/s]\n",
      "avg loss: 0.9078059874789042:  98%|█████████████████████████████████████████ | 14662/15000 [00:16<00:00, 729.56it/s]"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "for epoch in range(20):\n",
    "    learning_rate *= 0.9\n",
    "    print(epoch, learning_rate)\n",
    "    loop(train, learning_rate=learning_rate)\n",
    "    loop(validation)\n",
    "loop(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
