{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test data split\n",
    "\n",
    "import random\n",
    "from typing import TypeVar, List, Tuple\n",
    "X = TypeVar('X') # generic type to represent a data point\n",
    "\n",
    "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]:\n",
    "    \"\"\"Splits data into fractions [prob, 1 - prob]\"\"\"\n",
    "    \n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuflled list there\n",
    "\n",
    "data = [n for n in range(1000)]\n",
    "train, test = split_data(data, 0.75)\n",
    "\n",
    "# The proportion should be correct\n",
    "assert len(train) == 750\n",
    "assert len(test) == 250\n",
    "\n",
    "# And the original data should be preserved (in some order)\n",
    "assert sorted(train + test) == data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = TypeVar('Y') # generic type to represent output variables\n",
    "\n",
    "def train_test_split(xs: List[X], \n",
    "                     ys: List[Y], \n",
    "                     test_pct: float) -> Tuple[List[X], List[X], List[Y], List[Y]]:\n",
    "    \n",
    "    # Generate the indicies and split them\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "    \n",
    "    return([xs[i] for i in train_idxs], # x_train\n",
    "           [xs[i] for i in test_idxs],  # x_test\n",
    "           [ys[i] for i in train_idxs], # y_train\n",
    "           [ys[i] for i in test_idxs])  # y_test\n",
    "\n",
    "xs = [x for x in range(1000)]\n",
    "ys = [2 * x for x in xs]\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.25)\n",
    "\n",
    "# The proportion should be correct\n",
    "assert len(x_train) == len(y_train) == 750\n",
    "assert len(x_test) == len(y_test) == 250\n",
    "\n",
    "# Check that the corresponding data points are paired correctly\n",
    "assert all(y == 2 * x for x, y in zip(x_train, y_train))\n",
    "assert all(y == 2 * x for x, y in zip(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: fraction of correct predictions\n",
    "\n",
    "def accuracy(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    correct = tp + tn\n",
    "    total = tp + fp + fn + tn\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "assert accuracy(70, 4930, 13930, 981070) == 0.98114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision: how accurate our positive predictions are\n",
    "\n",
    "def precision(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    \n",
    "    return tp / (tp + fp)\n",
    "\n",
    "assert precision(70, 4930, 13930, 981070) == 0.014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall: fraction of the positives our model identified\n",
    "\n",
    "def recall(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    \n",
    "    return tp / (tp + fn)\n",
    "\n",
    "assert recall(70, 4930, 13930, 981070) == 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1\n",
    "\n",
    "def f1_score(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    p = precision(tp, fp, fn, tn)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "    \n",
    "    return 2 * p * r / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Precision-recall Tradeoff\n",
    "\n",
    "A model that predicts \"yes\" when it's even a little bit confident \n",
    "will probably have a high recall (increased TP; considering Spam only), but a low precision (increased FP).\n",
    "\n",
    "A model that predicts \"yes\" when it's extremely confident\n",
    "will probably have a low recall (decreased TP), but a high precision (increased TP).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bias-Variance Tradeoff\n",
    "\n",
    "Underfitted model will make a lot of mistakes for pretty much any training set (drawn from the sample population), \n",
    "which means that it has a high \"bias\". \n",
    "\n",
    "However, any two randomly chosen training sets should have pretty similar models,\n",
    "which means that it has a low \"variance\".\n",
    "\n",
    "So, underfitting typically yields high bias and low variance. \n",
    "-> We need to have more features that capture regularities in the data, otherwise more data won't help with bias.\n",
    "\n",
    "On the contrary, overfitting typically yields low bias and high variance.\n",
    "-> We need to remove features (decreasing model complexity) or more data to reduce variance\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
